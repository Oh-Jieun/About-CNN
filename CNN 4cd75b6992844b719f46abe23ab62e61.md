# CNN

# CNN(Convolutional Neural Networks) 구조

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled.png)

# Convolutional Layer

기본적으로 Convolution Layer에는 input값인 이미지와 필터(=합성곱 커널: convolution kernel)가 있다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%201.png)

위 그림처럼 필터(=kernel)가 이미지의 위에서부터 차례대로 내려오면서(sliding) dot products를 해나간다.

32 *  32 * 3 의 크기의 이미지에 5 * 5 * 3의 필터로 차례대로 훑고 내려오면서 5 * 5 * 3(=75)의 dot product 연산을 하는 것이다. 또한 각각의 dot product 결과로 하나의 숫자를 return한다. 즉 필터가 위치해있는 하나의 location 당 하나의 숫자로 표현되는 것이다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%202.png)

이 때 Convolution Layer(합성곱 층)의 뉴런은 입력 이미지의 모든 픽셀에 연결되는 것이 아니라, Convolution Layer 뉴런의 수용장 안에 있는 픽셀에만 연결된다.

CNN에서 필터의 크기(5 * 5 * 3)는 파라미터의 수를 의미하며 하나의 필터는 하나의 activation map(=feature map)을 생성한다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%203.png)

따라서 위와 같이 왼쪽의 input 이미지에 2개의 필터를 적용했다면 오른쪽 그림과 같이 2개의 activation maps가 생성된다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%204.png)

input으로 들어온 이미지를 activation의 관점에서 표현하면 오른쪽과 같이 새로운 형태의 이미지로 변환된다.

이 때 새로 생성된 activation maps의 depth는 적용한 필터의 수와 동일하다. 이렇게 생성된 activation map을 다음 Layer의 input값으로 전달된다. 

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%205.png)

32 * 32의 크기였던 input 이미지가 어떻게 28 * 28 로 변한 것일까?

# Stride

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%206.png)

input으로 7 * 7 크기의 이미지가 들어올 때 여기에 3 * 3 필터를 적용해보자.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%207.png)

**7 * 7 input**

**3 * 3 filter**

**stride 1**

**5 * 5 output**

stride는 한 필터와 다음 필터 사이의 간격이라고 생각하면 된다.

stride가 1이라는 것은 필터와 다음 필터의 간격이 1이라는 의미로, 1칸씩 가로, 세로 방향으로 이동하면서 이미지에 필터가 적용된다.

총 5칸을 이동할 수 있으므로 output으로는 5 * 5 크기의 activation map이 생성된다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%208.png)

**7 * 7 input**

**3 * 3 filter**

**stride 2**

**3 * 3 output**

stride 2 필터의 경우 7 * 7 이미지에 3 * 3 필터를 가지고 2칸씩 이동하면서 적용하면 3 * 3의 크기를 가진 activation map이 형성된다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%209.png)

이러한 원리로, output size에 대해 하나의 공식이 나온다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2010.png)

N은 이미지의 (w, h), F는 필터의 (w, h)를 의미한다.

만약 7 * 7 이미지에 3 * 3 필터를 stride 3으로 적용하면 (7 - 3) / 3 + 1 = 2.33

이동하는 칸이 부족한 것 → 이를 해결할 수 있는 방법으로 zero padding 기법이 있다.

# Zero Padding

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2011.png)

zero padding은 0의 값을 가진 픽셀로 해당 이미지를 둘러싸 주는 것으로, 이렇게 padding을 적용하면 7 * 7 이미지에 3 * 3 필터를 stride3으로 적용할 수 있게 된다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2012.png)

또한 padding을 이용하면 사이즈를 보존하는 효과를 얻을 수 있다.

### **padding을 이용해서 사이즈를 보존하는 것이 어떠한 의미를 가지며 왜 중요한가?**

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2013.png)

CNN에서는 Layer가 진행될수록 위의 그림처럼 사이즈가 점점 줄어들게 된다(shrinks volumes).

만약 volume이 빠르게 줄어들어 0이 되어 버리면 더 이상 CNN을 적용할 수 없는 상태가 된다. 이러한 문제를 해결하기 위해 padding을 이용하여 너무 빠르게 사이즈가 줄어들지 않게 보존해주는 것이다.

# Pooling Layer

CNN에서는 padding을 이용해서 사이즈를 보존해주되, 동시에 사이즈를 점점 줄여나가는 것이 중요하다.

따라서 Pooling Layer 에서는 down sampling을 담당한다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2014.png)

위와 같이 pooling layer에서 down sampling을 진행한 결과 사이즈가 절반으로 줄어들었음을 알 수 있다. 이러한 작업은 각 activation map에 독립적으로 적용되며 이 때 depth는 그대로 유지된다.

### Max Pooling

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2015.png)

Pooling 방법 중 가장 많이 쓰이는 것은 Max Pooling이다. 왼쪽 그림과 같이 4 * 4 이미지에 2 * 2 필터를 stride 2로 적용하여 빨강, 초록, 노랑, 파랑으로 각 영역을 구분해준다. 이 때 각 영역에서 가장 큰 값만 추출하는 방법이 Max Pooling이다.

그 결과 오른쪽 그림과 같이 각 영역에서 가장 큰 값들만 추출되어 down sampling되었음을 알 수 있다.

pooling layer에서는 convolution layer와 다르게 파라미터와 padding이 존재하지 않는다.

# Convolution Layer에서의 Hyperparameter

**Convolution Layer에는 4가지 하이퍼 파라미터가 있다.**

- 필터의 개수
- 필터의 크기
- stride
- zero padding

### Parameter Sharing

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2016.png)

동일한 depth 내의 뉴런들을 동일한 weight를 가지며 이것을 parameter sharing이라고 한다. 따라서 위의 그림에서 파란색 필터를 적용해서 나온 파란색 actiavation map에서는 parameter sharing을 하고 있는 것이다. 

같은 원리로 초록색 activation map 또한 내부적으로 같은 parameter를 적용한 것이다.

![Untitled](CNN%204cd75b6992844b719f46abe23ab62e61/Untitled%2017.png)

반면 별개의 activation map에 속하면서 동일한 위치에 있는 뉴런들의 경우 각각 다른 weight을 가진다.

즉 parameter sharing을 하지 않는다.